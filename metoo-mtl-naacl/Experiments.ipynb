{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Experiments.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Iv6bHn84jpUr"},"source":["**Dependencies and Libraries**"]},{"cell_type":"code","metadata":{"id":"VO08gB4Hh_S7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617966171623,"user_tz":-330,"elapsed":9218,"user":{"displayName":"puneet mathur","photoUrl":"","userId":"17185708312205506526"}},"outputId":"9c557e7d-798b-4f41-d35e-a5d76e59f98f"},"source":["import pandas as pd\n","from pathlib import Path\n","import re\n","import numpy as np\n","from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n","from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n","from sklearn.metrics import accuracy_score, hamming_loss\n","from scipy import sparse, stats\n","import pickle\n","import imblearn\n","from sklearn.utils import class_weight\n","from sklearn.model_selection import KFold, StratifiedKFold\n","import keras\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","from keras.layers import Activation, Dense, Conv1D, Lambda, Masking, Reshape, Concatenate, Bidirectional, Embedding, Input, GlobalMaxPooling1D, Convolution1D, MaxPooling1D, Dropout, Flatten, LSTM, TimeDistributed, concatenate\n","from keras.initializers import Constant\n","from keras.models import Model, Sequential\n","from keras.engine.topology import Layer\n","from keras import initializers, regularizers, constraints\n","from keras import optimizers\n","from keras import backend as K\n","import tensorflow as tf\n","from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n","seed = 3\n","np.random.seed(seed)\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","!pip install iterative-stratification"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n","  \"(https://pypi.org/project/six/).\", FutureWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n","  warnings.warn(message, FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Collecting iterative-stratification\n","  Downloading https://files.pythonhosted.org/packages/9d/79/9ba64c8c07b07b8b45d80725b2ebd7b7884701c1da34f70d4749f7b45f9a/iterative_stratification-0.1.6-py3-none-any.whl\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (1.19.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (0.22.2.post1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->iterative-stratification) (1.0.1)\n","Installing collected packages: iterative-stratification\n","Successfully installed iterative-stratification-0.1.6\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qKjJDNhIjvXR"},"source":["##**Single Task Learning Experiments:**\n","There are two sets of experiments here:\n","1. For choice of embeddings: GloVe v/s BERTweet (on MeToo tasks)\n","2. Single Task Learning for individual tasks (MeToo tasks and SemEval task) with the embedding performing better"]},{"cell_type":"markdown","metadata":{"id":"wxtaIWSIJN4-"},"source":["**The BERTweet embeddings for the tasks have been provided.**\n"," \n","**Please follow the guidelines present [here](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/JN4EYU) to retrieve the tweets for the MeToo dataset and [this](https://github.com/VinAIResearch/BERTweet) repository to explore BERTweet**"]},{"cell_type":"code","metadata":{"id":"fQ5w9GZ47BlI","colab":{"base_uri":"https://localhost:8080/","height":443},"executionInfo":{"status":"error","timestamp":1617966269209,"user_tz":-330,"elapsed":1777,"user":{"displayName":"puneet mathur","photoUrl":"","userId":"17185708312205506526"}},"outputId":"c6f47969-0628-45f9-c109-cd5eb681090d"},"source":["# BERTweet embeddings, Tweet IDs, and Labels for the MeToo tasks\n","stance = pd.read_csv('Embeddings/stance_bt.csv')\n","hate_speech = pd.read_csv('Embeddings/hatespeech_bt.csv')\n","sarcasm = pd.read_csv('Embeddings/sarcasm_bt.csv')\n","dialogue = pd.read_csv('Embedings/dialogue_bt.csv')"],"execution_count":2,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-d9626ae0e439>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# BERTweet embeddings, Tweet IDs, and Labels for the MeToo tasks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Embeddings/stance_bt.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mhate_speech\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Embeddings/hatespeech_bt.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msarcasm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Embeddings/sarcasm_bt.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdialogue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Embedings/dialogue_bt.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Embeddings/stance_bt.csv'"]}]},{"cell_type":"markdown","metadata":{"id":"0KNZX2C-4YM9"},"source":["*Imbalancy check in individual tasks*"]},{"cell_type":"code","metadata":{"id":"dCjqMKNev9n4"},"source":["fig_size = plt.rcParams[\"figure.figsize\"]\n","fig_size[0] = 10\n","fig_size[1] = 8\n","plt.rcParams[\"figure.figsize\"] = fig_size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XCRTX-TZwQy4"},"source":["stance_labels = stance[['Support','Oppose','None']]\n","y_stance = np.argmax(stance_labels.values,axis=1)\n","stance_labels.sum(axis=0).plot.bar()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2EZKU9U-wFmQ"},"source":["hs_labels = hate_speech[['Directed_Hate','Generalized_Hate','None']]\n","y_hs = np.argmax(hs_labels.values,axis=1)\n","hs_labels.sum(axis=0).plot.bar()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"goYqXbrvwZ6G"},"source":["sarcasm_labels = sarcasm[['Sarcasm','Not']]\n","y_sar = np.argmax(sarcasm_labels.values,axis=1)\n","sarcasm_labels.sum(axis=0).plot.bar()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RsxIL4H9wdpj"},"source":["dialogue_labels = dialogue[['Allegation','Justification','Refutation','None']]\n","y_dia = np.argmax(dialogue_labels.values,axis=1)\n","dialogue_labels.sum(axis=0).plot.bar()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TVAuCwOVfFA2"},"source":["# BERTweet features\n","metoo_bt = pd.read_csv('Embeddings/stance_bt.csv')\n","X_metoo_bt = []\n","for ind in metoo_bt.index:\n","  X_metoo_bt.append(metoo_bt['bert_tweet'][ind])\n","X_metoo_bt = np.asarray(X_metoo_bt)\n","input_shape_metoo_bt = X_metoo_bt[0].shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kRm36G1SK2lE"},"source":["**The following code to prepare GloVe-Twitter embedding will work only with access to the MeToo tweets.**\n","\n","**Please follow the guide [here](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/JN4EYU) to retrieve the tweets**"]},{"cell_type":"code","metadata":{"id":"DyI0wTuHrJQQ"},"source":["# Preparing GLoVe Embeddings\n","\n","# Retrive the complete GLoVe-twitter pre-trained vectors from the official database\n","!wget http://nlp.stanford.edu/data/glove.twitter.27B.zip\n","!unzip glove.twitter.27B.zip\n","\n","# We use the one with dimension 200 for our case\n","path = \"glove.twitter.27B.200d.txt\"\n","embeddings_index = {}\n","with open(path) as f:\n","   for line in tqdm(f):\n","      values = line.split()\n","      word = values[0]\n","      coefs = np.asarray(values[1:], dtype='float32')\n","      embeddings_index[word] = coefs\n","del coefs, word, values\n","print('Found %s word vectors.' % len(embeddings_index))\n","\n","# Preprocess the tweets and convert into features based on GLoVe vector space\n","MAX_NUM_WORDS = 32928\n","text = metoo['tweet'].values\n","tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n","tokenizer.fit_on_texts(text)\n","sequences = tokenizer.texts_to_sequences(text)\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","MAX_SEQUENCE_LENGTH = max(map(lambda x:len(x.split()), text))\n","print (MAX_SEQUENCE_LENGTH)\n","X_metoo_glove = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","print('Preparing embedding matrix.')\n","EMBEDDING_DIM = 200\n","# prepare embedding matrix\n","num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n","embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n","for word, i in word_index.items():\n","    if i > MAX_NUM_WORDS:\n","        continue\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        # words not found in embedding index will be all-zeros.\n","        embedding_matrix[i] = embedding_vector"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oqSTRDz9f8gq"},"source":["# Using focal loss due to high imbalance in classes\n","def categorical_focal_loss(gamma=2.0, alpha=0.25):\n","    \"\"\"\n","    Implementation of Focal Loss from the paper in multiclass classification\n","    Formula:\n","        loss = -alpha*((1-p)^gamma)*log(p)\n","    Parameters:\n","        alpha -- the same as wighting factor in balanced cross entropy\n","        gamma -- focusing parameter for modulating factor (1-p)\n","    Default value:\n","        gamma -- 2.0 as mentioned in the paper\n","        alpha -- 0.25 as mentioned in the paper\n","    \"\"\"\n","    def focal_loss(y_true, y_pred):\n","        # Define epsilon so that the backpropagation will not result in NaN\n","        # for 0 divisor case\n","        epsilon = K.epsilon()\n","        # Add the epsilon to prediction value\n","        #y_pred = y_pred + epsilon\n","        # Clip the prediction value\n","        y_pred = K.clip(y_pred, epsilon, 1.0-epsilon)\n","        # Calculate cross entropy\n","        cross_entropy = -y_true*K.log(y_pred)\n","        # Calculate weight that consists of  modulating factor and weighting factor\n","        weight = alpha * y_true * K.pow((1-y_pred), gamma)\n","        # Calculate focal loss\n","        loss = weight * cross_entropy\n","        # Sum the losses in mini_batch\n","        loss = K.sum(loss, axis=1)\n","        return loss\n","    return focal_loss\n","\n","def binary_focal_loss(gamma=2.0, alpha=0.25):\n","    \"\"\"\n","    Implementation of Focal Loss from the paper in multiclass classification\n","    Formula:\n","        loss = -alpha_t*((1-p_t)^gamma)*log(p_t)\n","        \n","        p_t = y_pred, if y_true = 1\n","        p_t = 1-y_pred, otherwise\n","        \n","        alpha_t = alpha, if y_true=1\n","        alpha_t = 1-alpha, otherwise\n","        \n","        cross_entropy = -log(p_t)\n","    Parameters:\n","        alpha -- the same as wighting factor in balanced cross entropy\n","        gamma -- focusing parameter for modulating factor (1-p)\n","    Default value:\n","        gamma -- 2.0 as mentioned in the paper\n","        alpha -- 0.25 as mentioned in the paper\n","    \"\"\"\n","    def focal_loss(y_true, y_pred):\n","        # Define epsilon so that the backpropagation will not result in NaN\n","        # for 0 divisor case\n","        epsilon = K.epsilon()\n","        # Add the epsilon to prediction value\n","        #y_pred = y_pred + epsilon\n","        # Clip the prediciton value\n","        y_pred = K.clip(y_pred, epsilon, 1.0-epsilon)\n","        # Calculate p_t\n","        p_t = tf.where(K.equal(y_true, 1), y_pred, 1-y_pred)\n","        # Calculate alpha_t\n","        alpha_factor = K.ones_like(y_true)*alpha\n","        alpha_t = tf.where(K.equal(y_true, 1), alpha_factor, 1-alpha_factor)\n","        # Calculate cross entropy\n","        cross_entropy = -K.log(p_t)\n","        weight = alpha_t * K.pow((1-p_t), gamma)\n","        # Calculate focal loss\n","        loss = weight * cross_entropy\n","        # Sum the losses in mini_batch\n","        loss = K.sum(loss, axis=1)\n","        return loss   \n","    return focal_loss    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8LBkH_tlphJ_"},"source":["*Modeling and Evaluation*"]},{"cell_type":"code","metadata":{"id":"I1erfFyjsZwS"},"source":["def get_model(n_classes = 3, emb = 'bt'):\n","  if n_classes == 2:\n","    activation = 'sigmoid'\n","  else:\n","    activation = 'softmax'\n","  \n","  # GloVe Input\n","  embedding_layer = Embedding(len(word_index)+1,\n","                              EMBEDDING_DIM,\n","                              embeddings_initializer=Constant(embedding_matrix),\n","                              input_length=MAX_SEQUENCE_LENGTH,\n","                              trainable=False)\n","\n","  sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n","  input_glove = embedding_layer(sequence_input)\n","\n","  # BERTweet Input\n","  input_bt = Input(shape=input_shape_metoo_bt)\n","\n","  if emb == 'glove':\n","    x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.4,recurrent_dropout=0.4))(input_glove)\n","  elif emb == 'bt':\n","    x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.4,recurrent_dropout=0.4))(input_bt)\n","  \n","  x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.4,recurrent_dropout=0.4))(x)\n","  x = Dropout(0.3)(x)\n","  x = Bidirectional(LSTM(128, return_sequences=False, dropout=0.4,recurrent_dropout=0.4))(x)\n","  x = Dropout(0.3)(x)\n","  x = Dense(128, activation=\"relu\")(x)\n","  x = Dense(n_classes, activation=activation)(x)\n","  \n","  if emb == 'glove':\n","    model = Model(inputs=sequence_input, outputs=x)\n","  elif emb == 'bt':\n","    model = Model(inputs=input_bt, outputs=x)\n","  \n","  opt = keras.optimizers.Adam(learning_rate=0.001)\n","  if n_classes == 2:\n","    model.compile(loss=binary_focal_loss(gamma=2.0, alpha=0.25), optimizer=opt, metrics=['acc'])\n","  else:\n","    model.compile(loss=categorical_focal_loss(gamma=2.0, alpha=0.25), optimizer=opt, metrics=['acc'])\n","  print(model.summary())\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"39-nZbTAspk-"},"source":["def get_l1_train_test(X, Y, n_classes = 3, k = 5, task = 'stance', emb = 'bt'):\n","    cv_object = StratifiedKFold(n_splits=k, shuffle=False, random_state = None)\n","    \n","    F1_macro = []\n","    P_macro = []\n","    R_macro = []\n","    F1_micro = []\n","    P_micro = []\n","    R_micro = []\n","    F1_weighted = []\n","    P_weighted = []\n","    R_weighted = []\n","    fold = 0\n","    for train_index, test_index in cv_object.split(X, Y):\n","        fold = fold + 1\n","        print (\"Fold \",fold,\":\")\n","        X_train, y_train = X[train_index], Y[train_index]\n","        X_test, y_test = X[test_index], Y[test_index]\n","        model = get_model(n_classes = n_classes, emb = emb)\n","        y_train = np.eye(n_classes)[y_train]\n","        history = model.fit(X_train, y_train, epochs=20, batch_size=128, verbose=1)\n","        plt.plot(history.history['acc'])\n","\n","        plt.title('model accuracy')\n","        plt.ylabel('accuracy')\n","        plt.xlabel('epoch')\n","        plt.legend(['train'], loc='upper left')\n","        plt.show()\n","\n","        plt.plot(history.history['loss'])\n","\n","        plt.title('model loss')\n","        plt.ylabel('loss')\n","        plt.xlabel('epoch')\n","        plt.legend(['train'], loc='upper left')\n","        plt.show()\n","        probs = model.predict(X_test, batch_size=128, verbose=1)\n","        preds = np.argmax(probs, axis=1)\n","        f1_score_macro = metrics.f1_score(y_test, preds, average='macro')\n","        p_score_macro = metrics.precision_score(y_test, preds, average='macro')\n","        r_score_macro = metrics.recall_score(y_test, preds, average='macro')\n","        f1_score_micro = metrics.f1_score(y_test, preds, average='micro')\n","        p_score_micro = metrics.precision_score(y_test, preds, average='micro')\n","        r_score_micro = metrics.recall_score(y_test, preds, average='micro')\n","        f1_score_weighted = metrics.f1_score(y_test, preds, average='weighted')\n","        p_score_weighted = metrics.precision_score(y_test, preds, average='weighted')\n","        r_score_weighted = metrics.recall_score(y_test, preds, average='weighted')\n","        print (\"F1 Macro: \",f1_score_macro, \" P Macro: \", p_score_macro, \" R Macro: \",r_score_macro)\n","        print (\"F1 Micro: \",f1_score_micro, \" P Micro: \", p_score_micro, \" R Micro: \",r_score_micro)\n","        print (\"F1 Weighted: \",f1_score_weighted, \" P Weighted: \", p_score_weighted, \" R Weighted: \",r_score_weighted)\n","        print (metrics.confusion_matrix(y_test, preds))\n","        print (metrics.classification_report(y_test, preds))\n","        F1_macro.append(f1_score_macro)\n","        P_macro.append(p_score_macro)\n","        R_macro.append(r_score_macro)\n","        F1_micro.append(f1_score_micro)\n","        P_micro.append(p_score_micro)\n","        R_micro.append(r_score_micro)\n","        F1_weighted.append(f1_score_weighted)\n","        P_weighted.append(p_score_weighted)\n","        R_weighted.append(r_score_weighted)\n","\n","    print (\" Macro - Mean and Dev-  F1: \", np.mean(F1_macro),\"(\",np.std(F1_macro),\") P: \",np.mean(P_macro),\" (\",np.std(P_macro),\") R: \",np.mean(R_macro),\" (\",np.std(R_macro),\")\")\n","    print (\" Micro -  Mean and Dev-  F1: \", np.mean(F1_micro),\"(\",np.std(F1_micro),\") P: \",np.mean(P_micro),\" (\",np.std(P_micro),\") R: \",np.mean(R_micro),\" (\",np.std(R_micro),\")\")\n","    print (\" Weighted - Mean and Dev-  F1: \", np.mean(F1_weighted),\"(\",np.std(F1_weighted),\") P: \",np.mean(P_weighted),\" (\",np.std(P_weighted),\") R: \",np.mean(R_weighted),\" (\",np.std(R_weighted),\")\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4oJkOw6t7Vsx"},"source":["if task == 'stance':\n","  label = y_stance\n","  n_classes = 3\n","elif task == 'hatespeech':\n","  label = y_hs\n","  n_classes = 3\n","elif task == 'dialogue':\n","  label = y_dia\n","  n_classes = 4\n","elif task == 'sarcasm':\n","  label = y_sar\n","  n_classes = 2\n","\n","if emb == 'glove':\n","  data = X_metoo_glove\n","elif emb == 'bt':\n","  data = X_metoo_bt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nrgrAXnJs2s0"},"source":["# Sample evaluation for 'STANCE' Classification with 'BERTweet' embeddings. Change the inputs accordingly for different tasks and embeddings\n","task = 'stance'\n","emb = 'bt'\n","get_l1_train_test(data, label, n_classes = n_classes, k = 5, task = task, emb = emb)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RabILFRXqyhr"},"source":["**We found BERTweet performing better than GLoVe embeddings.**\n","**We therefore now use it for the emotion recognition task as well**"]},{"cell_type":"code","metadata":{"id":"TZPsZtORqy_K"},"source":["# BERTweet embeddings and labels are provided for complete dataset. For tweets, please refer the official website for SemEval 2018 Competition. \n","emo_bt = pd.read_csv('Embeddings/emo_bt.csv')\n","X_emo_bt = []\n","for ind in emo_bt.index:\n","    X_emo_bt.append(emo_bt['bert_tweet'][ind])\n","\n","X_emo_bt = np.asarray(X_emo_bt)\n","input_shape_emo_bt = X_emo_bt[0].shape\n","\n","emo_labels = emo_bt[['anger','anticipation','disgust','fear','joy','love','optimism','pessimism','sadness','surprise','trust']]\n","emo_labels.sum(axis=0).plot.bar()\n","y_emo = emo_labels.values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GpaXWRnDDa-y"},"source":["*Modeling and Evaluation*"]},{"cell_type":"code","metadata":{"id":"FS6_3N6s_Bue"},"source":["def get_model():\n","\n","  input_bt = Input(shape=input_shape_emo_bt)\n","  x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.4,recurrent_dropout=0.4))(input_bt) \n","  x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.4,recurrent_dropout=0.4))(x)\n","  x = Dropout(0.3)(x)\n","  x = Bidirectional(LSTM(128, return_sequences=False, dropout=0.4,recurrent_dropout=0.4))(x)\n","  x = Dropout(0.3)(x)\n","  x = Dense(128, activation=\"relu\")(x)\n","  x = Dense(11, activation='sigmoid')(x)\n","  model = Model(inputs=input_bt, outputs=x)\n","  opt = keras.optimizers.Adam(learning_rate=0.001)\n","  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['acc'])\n","  print(model.summary())\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KngyHI_xBZn6"},"source":["from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n","\n","def get_l1_train_test(X, Y, k=5,task='emotion_stl'):\n","    cv_object = MultilabelStratifiedKFold(n_splits=k, random_state=None)\n","    F1_macro = []\n","    P_macro = []\n","    R_macro = []\n","    F1_micro = []\n","    P_micro = []\n","    R_micro = []\n","    F1_weighted = []\n","    P_weighted = []\n","    R_weighted = []\n","    fold = 0\n","    for train_index, test_index in cv_object.split(X, Y):\n","        fold = fold + 1\n","        epochs = 20\n","        batch_size = 128\n","        print (\"Fold \",fold,\":\")\n","        X_train, y_train = X[train_index], Y[train_index]\n","        X_test, y_test = X[test_index], Y[test_index]\n","        \n","        model = get_model()\n","        history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n","        \n","        plt.plot(history.history['acc'])\n","        plt.title('model accuracy')\n","        plt.ylabel('accuracy')\n","        plt.xlabel('epoch')\n","        plt.legend(['train'], loc='upper left')\n","        plt.show()\n","\n","        plt.plot(history.history['loss'])\n","        plt.title('model loss')\n","        plt.ylabel('loss')\n","        plt.xlabel('epoch')\n","        plt.legend(['train'], loc='upper left')\n","        plt.show()\n","\n","        probs = model.predict(X_test, batch_size=batch_size, verbose=1)\n","        preds = (probs >= 0.5).astype(int)\n","        f1_score_macro = metrics.f1_score(y_test, preds, average='macro')\n","        p_score_macro = metrics.precision_score(y_test, preds, average='macro')\n","        r_score_macro = metrics.recall_score(y_test, preds, average='macro')\n","        f1_score_micro = metrics.f1_score(y_test, preds, average='micro')\n","        p_score_micro = metrics.precision_score(y_test, preds, average='micro')\n","        r_score_micro = metrics.recall_score(y_test, preds, average='micro')\n","        f1_score_weighted = metrics.f1_score(y_test, preds, average='weighted')\n","        p_score_weighted = metrics.precision_score(y_test, preds, average='weighted')\n","        r_score_weighted = metrics.recall_score(y_test, preds, average='weighted')\n","        print (\"F1 Macro: \",f1_score_macro, \" P Macro: \", p_score_macro, \" R Macro: \",r_score_macro)\n","        print (\"F1 Micro: \",f1_score_micro, \" P Micro: \", p_score_micro, \" R Micro: \",r_score_micro)\n","        print (\"F1 Weighted: \",f1_score_weighted, \" P Weighted: \", p_score_weighted, \" R Weighted: \",r_score_weighted)\n","        print(\"Hamming loss = \",hamming_loss(y_test,preds))\n","        print (metrics.classification_report(y_test, preds))\n","        F1_macro.append(f1_score_macro)\n","        P_macro.append(p_score_macro)\n","        R_macro.append(r_score_macro)\n","        F1_micro.append(f1_score_micro)\n","        P_micro.append(p_score_micro)\n","        R_micro.append(r_score_micro)\n","        F1_weighted.append(f1_score_weighted)\n","        P_weighted.append(p_score_weighted)\n","        R_weighted.append(r_score_weighted)\n","\n","    print (\" Macro - Mean and Dev-  F1: \",np.mean(F1_macro),\"(\",np.std(F1_macro),\") P: \",np.mean(P_macro),\" (\",np.std(P_macro),\") R: \",np.mean(R_macro),\" (\",np.std(R_macro),\")\")\n","    print (\" Micro -  Mean and Dev-  F1: \",np.mean(F1_micro),\"(\",np.std(F1_micro),\") P: \",np.mean(P_micro),\" (\",np.std(P_micro),\") R: \",np.mean(R_micro),\" (\",np.std(R_micro),\")\")\n","    print (\" Weighted - Mean and Dev-  F1: \",np.mean(F1_weighted),\"(\",np.std(F1_weighted),\") P: \",np.mean(P_weighted),\" (\",np.std(P_weighted),\") R: \",np.mean(R_weighted),\" (\",np.std(R_weighted),\")\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xbk8ewE0Cz6w"},"source":["get_l1_train_test(X_emo_bt, y_emo, k=5, task='emotion_stl')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YktwKZ1tF5gZ"},"source":["##**Multi Task Learning Experiments**\n","1. Homogeneous MTL (Between MeToo tasks pairwise)\n","2. Heterogeneous MTL (Between individual MeToo and Emotion Classification task)"]},{"cell_type":"markdown","metadata":{"id":"wOu6kZLQGlTO"},"source":["**Homogeneous MTL**"]},{"cell_type":"code","metadata":{"id":"PqUvkli1EffY"},"source":["# We will create and save the crossfolds prior to training so as to decrease computation load during training. Please create a folder to save these (in this case, named 'MeTooFolds')\n","\n","metoo_object = StratifiedKFold(n_splits=5, shuffle=False, random_state = None)\n","fold = 0\n","train_indices_stance = []\n","test_indices_stance =  []\n","for train_index, test_index in metoo_object.split(X_metoo_bt, y_stance):\n","  fold = fold + 1\n","  print (\"Fold \",fold,\":\")\n","  train_indices_stance.append([train_index])\n","  test_indices_stance.append([test_index])\n","  X_train, y_train_stance,y_train_hs,y_train_sar,y_train_dia  = X_metoo_bt[train_index], y_stance[train_index],y_hs[train_index], y_sar[train_index], y_dia[train_index]\n","  np.save('MeTooFolds/X_train_metoo_'+str(fold)+'.npy',X_train)\n","  np.save('MeTooFolds/Y_train_stance_'+str(fold)+'.npy',y_train_stance)\n","  np.save('MeTooFolds/Y_train_hs_'+str(fold)+'.npy',y_train_hs)\n","  np.save('MeTooFolds/Y_train_sar_'+str(fold)+'.npy',y_train_sar)\n","  np.save('MeTooFolds/Y_train_dia_'+str(fold)+'.npy',y_train_dia)\n","  \n","  del(X_train)\n","  del(y_train_stance)\n","  del(y_train_hs)\n","  del(y_train_sar)\n","  del(y_train_dia)\n","\n","\n","  X_test, y_test_stance,y_test_hs,y_test_sar,y_test_dia = X_metoo_bt[test_index], y_stance[test_index],y_hs[test_index],y_sar[test_index],y_dia[test_index]\n","  np.save('MeTooFolds/X_test_metoo_'+str(fold)+'.npy',X_test)\n","  np.save('MeTooFolds/Y_test_stance_'+str(fold)+'.npy',y_test_stance)\n","  np.save('MeTooFolds/Y_test_hs_'+str(fold)+'.npy',y_test_hs)\n","  np.save('MeTooFolds/Y_test_sar_'+str(fold)+'.npy',y_test_sar)\n","  np.save('MeTooFolds/Y_test_dia_'+str(fold)+'.npy',y_test_dia)\n","\n","  del(X_test)\n","  del(y_test_stance)\n","  del(y_test_hs)\n","  del(y_test_sar)\n","  del(y_test_dia)\n","\n","\n","train_indices_stance = np.asarray(train_indices_stance)\n","np.save('MeTooFolds/train_indices_metoo.npy',train_indices_stance)\n","\n","test_indices_stance = np.asarray(test_indices_stance)\n","np.save('MeTooFolds/test_indices_metoo.npy',test_indices_stance)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ft8tCIi_X0Qm"},"source":["*Modeling and Evaluation*"]},{"cell_type":"code","metadata":{"id":"ckUNP9JWGfH2"},"source":["def get_model(task1, task2, n1_classes=3, n2_classes=4, lw1=0.8):\n","\n","    if n1_classes == 2:\n","      activation1 = 'sigmoid'\n","    else:\n","      activation1 = 'softmax'\n","\n","    if n2_classes == 2:\n","      activation2 = 'sigmoid'\n","    else:\n","      activation2 = 'softmax'\n","      \n","    lw1 = lw1\n","    lw2 = 1-lw1\n","    input_bt = Input(shape=(117,768))\n","    x=Bidirectional(LSTM(units=128, dropout=0.4, recurrent_dropout=0.4, activation='tanh',return_sequences=True))(input_bt)\n","    x=Bidirectional(LSTM(units=128, dropout=0.4, recurrent_dropout=0.4, activation='tanh',return_sequences=True))(x)\n","    x = Dropout(0.3)(x)\n","\n","    t1 = Bidirectional(LSTM(units=256, dropout=0.4, recurrent_dropout=0.4, activation='tanh',return_sequences=False))(x)\n","    t2 = Bidirectional(LSTM(units=256, dropout=0.4, recurrent_dropout=0.4, activation='tanh',return_sequences=False))(x)\n","    t1 = Dropout(0.3)(t1)\n","    t2 = Dropout(0.3)(t2)\n","\n","    fc1 = Dense(128, activation=\"relu\")(t1)\n","    fc2 = Dense(128, activation=\"relu\")(t2)\n","\n","    final1 = Dense(3, activation='softmax',name = task1)(fc1)\n","    final2 = Dense(4, activation='softmax',name = task2)(fc2)\n","    model = Model(inputs=input_bt, outputs=[final1,final2])\n","    \n","    opt = keras.optimizers.Adam(learning_rate=0.001,beta_1=0.9, beta_2=0.999, amsgrad=False)\n","\n","    if n1_classes == 2:\n","      model.compile(loss={task1: binary_focal_loss(gamma=2.0, alpha=0.25),task2: categorical_focal_loss(gamma=2.0, alpha=0.25)} , optimizer=opt, metrics=['acc'],loss_weights={task1: lw1, task2: lw2})\n","    elif n2_classes == 2:\n","      model.compile(loss={task1: categorical_focal_loss(gamma=2.0, alpha=0.25),task2: binary_focal_loss(gamma=2.0, alpha=0.25)} , optimizer=opt, metrics=['acc'],loss_weights={task1: lw1, task2: lw2})\n","    else:\n","      model.compile(loss={task1: categorical_focal_loss(gamma=2.0, alpha=0.25),task2: categorical_focal_loss(gamma=2.0, alpha=0.25)} , optimizer=opt, metrics=['acc'],loss_weights={task1: lw1, task2: lw2})\n","    \n","    print(model.summary())\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N6kQdxF2OjHt"},"source":["def get_l1_train_test(k=5, task1='stance', task2='hatespeech', n1_classes=3, n2_claases=4, lw1 = 0.8):\n","    \n","    F1_macro1 = []\n","    P_macro1= []\n","    R_macro1 = []\n","    F1_micro1= []\n","    P_micro1= []\n","    R_micro1= []\n","    F1_weighted1 = []\n","    P_weighted1= []\n","    R_weighted1= []\n","\n","    F1_macro2 = []\n","    P_macro2= []\n","    R_macro2 = []\n","    F1_micro2 = []\n","    P_micro2 = []\n","    R_micro2 = []\n","    F1_weighted2 = []\n","    P_weighted2= []\n","    R_weighted2= []\n","    fold = 0\n","    for j in range(k):\n","        \n","        fold = fold + 1\n","        epochs = 20\n","        batch_size = 128\n","        print (\"Fold \",fold,\":\")\n","        X_train, y_train1 ,y_train2  =   np.load('MeTooFolds/X_train_metoo_'+str(fold)+'.npy') ,  np.load('MeTooFolds/Y_train_'+str(task1)+'_'+str(fold)+'.npy'),  np.load('MeTooFolds/Y_train_'+str(task2)+'_'+str(fold)+'.npy')\n","        X_test, y_test1 ,y_test2  =   np.load('MeTooFolds/X_test_metoo_'+str(fold)+'.npy') ,  np.load('MeTooFolds/Y_test_'+str(task1)+'_'+str(fold)+'.npy'),  np.load('MeTooFolds/Y_test_'+str(task2)+'_'+str(fold)+'.npy')\n","\n","        model = get_model(task1=task1, task2=task2, n1_classes=n1_classes, n2_classes=n2_classes, lw1=lw1)\n","        y_train1 = np.eye(n1_classes)[y_train1]\n","        y_train2 = np.eye(n2_classes)[y_train2]\n","        history = model.fit(X_train, [y_train1,y_train2], epochs=epochs, batch_size = batch_size, verbose=1)\n","        \n","        probs1,probs2 = model.predict(X_test, batch_size=batch_size, verbose=1)\n","        preds1 = np.argmax(probs1, axis=1)\n","        preds2 = np.argmax(probs2, axis=1)\n","\n","        print(\"METRICS FOR TASK 1:\" + str(task1))\n","        f1_score_macro1 = metrics.f1_score(y_test1, preds1, average='macro')\n","        p_score_macro1 = metrics.precision_score(y_test1, preds1, average='macro')\n","        r_score_macro1 = metrics.recall_score(y_test1, preds1, average='macro')\n","        f1_score_micro1 = metrics.f1_score(y_test1, preds1, average='micro')\n","        p_score_micro1 = metrics.precision_score(y_test1, preds1, average='micro')\n","        r_score_micro1 = metrics.recall_score(y_test1, preds1, average='micro')\n","        f1_score_weighted1 = metrics.f1_score(y_test1, preds1, average='weighted')\n","        p_score_weighted1 = metrics.precision_score(y_test1, preds1, average='weighted')\n","        r_score_weighted1 = metrics.recall_score(y_test1, preds1, average='weighted')\n","        print (\"F1 Macro1: \",f1_score_macro1, \" P Macro1: \", p_score_macro1, \" R Macro1: \",r_score_macro1)\n","        print (\"F1 Micro1: \",f1_score_micro1, \" P Micro1: \", p_score_micro1, \" R Micro1: \",r_score_micro1)\n","        print (\"F1 Weighted1: \",f1_score_weighted1, \" P Weighted1: \", p_score_weighted1, \" R Weighted1: \",r_score_weighted1)\n","        \n","        print (metrics.confusion_matrix(y_test1, preds1))\n","        print (metrics.classification_report(y_test1, preds1))\n","        F1_macro1.append(f1_score_macro1)\n","        P_macro1.append(p_score_macro1)\n","        R_macro1.append(r_score_macro1)\n","        F1_micro1.append(f1_score_micro1)\n","        P_micro1.append(p_score_micro1)\n","        R_micro1.append(r_score_micro1)\n","        F1_weighted1.append(f1_score_weighted1)\n","        P_weighted1.append(p_score_weighted1)\n","        R_weighted1.append(r_score_weighted1)\n","\n","        print(\"METRICS FOR TASK 2:\" + str(task2))\n","        f1_score_macro2 = metrics.f1_score(y_test2, preds2, average='macro')\n","        p_score_macro2 = metrics.precision_score(y_test2, preds2, average='macro')\n","        r_score_macro2 = metrics.recall_score(y_test2, preds2, average='macro')\n","        f1_score_micro2 = metrics.f1_score(y_test2, preds2, average='micro')\n","        p_score_micro2 = metrics.precision_score(y_test2, preds2, average='micro')\n","        r_score_micro2 = metrics.recall_score(y_test2, preds2, average='micro')\n","        f1_score_weighted2 = metrics.f1_score(y_test2, preds2, average='weighted')\n","        p_score_weighted2 = metrics.precision_score(y_test2, preds2, average='weighted')\n","        r_score_weighted2 = metrics.recall_score(y_test2, preds2, average='weighted')\n","        print (\"F1 Macro2: \",f1_score_macro2, \" P Macro2: \", p_score_macro2, \" R Macro2: \",r_score_macro2)\n","        print (\"F1 Micro2: \",f1_score_micro2, \" P Micro2: \", p_score_micro2, \" R Micro2: \",r_score_micro2)\n","        print (\"F1 Weighted2: \",f1_score_weighted2, \" P Weighted2: \", p_score_weighted2, \" R Weighted2: \",r_score_weighted2)\n","\n","        print (metrics.confusion_matrix(y_test2, preds2))\n","        print (metrics.classification_report(y_test2, preds2))\n","        F1_macro2.append(f1_score_macro2)\n","        P_macro2.append(p_score_macro2)\n","        R_macro2.append(r_score_macro2)\n","        F1_micro2.append(f1_score_micro2)\n","        P_micro2.append(p_score_micro2)\n","        R_micro2.append(r_score_micro2)\n","        F1_weighted2.append(f1_score_weighted2)\n","        P_weighted2.append(p_score_weighted2)\n","        R_weighted2.append(r_score_weighted2)\n","\n","        del(X_train)\n","        del(X_test)\n","        del(y_train1)\n","        del(y_train2)\n","        del(y_test1)\n","        del(y_test2)\n","\n","    print('FINAL RESULTS FOR TASK 1:'+str(task1))\n","    print (\" Macro - Mean and Dev-  F1: \",np.mean(F1_macro1),\"(\",np.std(F1_macro1),\") P: \",np.mean(P_macro1),\" (\",np.std(P_macro1),\") R: \",np.mean(R_macro1),\" (\",np.std(R_macro1),\")\")\n","    print (\" Micro -  Mean F1 Dev-  F1: \",np.mean(F1_micro1),\"(\",np.std(F1_micro1),\") P: \",np.mean(P_micro1),\" (\",np.std(P_micro1),\") R: \",np.mean(R_micro1),\" (\",np.std(R_micro1),\")\")\n","    print (\" Weighted - Mean and Dev-  F1: \",np.mean(F1_weighted1),\"(\",np.std(F1_weighted1),\") P: \",np.mean(P_weighted1),\" (\",np.std(P_weighted1),\") R: \",np.mean(R_weighted1),\" (\",np.std(R_weighted1),\")\")\n","\n","    print('FINAL RESULTS FOR TASK 2:'+str(task2))\n","    print (\" Macro - Mean and Dev-  F1: \",np.mean(F1_macro2),\"(\",np.std(F1_macro2),\") P: \",np.mean(P_macro2),\" (\",np.std(P_macro2),\") R: \",np.mean(R_macro2),\" (\",np.std(R_macro2),\")\")\n","    print (\" Micro -  Mean F1 Dev-  F1: \",np.mean(F1_micro2),\"(\",np.std(F1_micro2),\") P: \",np.mean(P_micro2),\" (\",np.std(P_micro2),\") R: \",np.mean(R_micro2),\" (\",np.std(R_micro2),\")\")\n","    print (\" Weighted - Mean and Dev-  F1: \",np.mean(F1_weighted2),\"(\",np.std(F1_weighted2),\") P: \",np.mean(P_weighted2),\" (\",np.std(P_weighted2),\") R: \",np.mean(R_weighted2),\" (\",np.std(R_weighted2),\")\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g9ZYSWj8QG6v"},"source":["if task1 == 'stance' or task1 == 'hatespeech':\n","  n1_classes = 3\n","elif task1 == 'sarcasm':\n","  n1_classes == 2\n","elif task1 == 'dialogue':\n","  n1_classes = 4\n","\n","if task2 == 'stance' or task2 == 'hatespeech':\n","  n2_classes = 3\n","elif task2 == 'sarcasm':\n","  n2_classes == 2\n","elif task2 == 'dialogue':\n","  n2_classes = 4\n","\n","# Sample evaluation for 'STANCE' (with loss weight 0.8) and 'DIALOGUE' (with loss weight 0.2) multi task learning. Change the inputs accordingly for different tasks and loss weights\n","task1 = 'stance'\n","task2 = 'dialogue'\n","lw1 = 0.8\n","get_l1_train_test(k=5, task1=task1, task2=task2, n1_classes=n1_classes, n2_classes=n2_classes, lw1=lw1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A7osi044X4SX"},"source":["**Heterogeneous MTL**"]},{"cell_type":"code","metadata":{"id":"pQqiE-fJX7Ba"},"source":["# Create a folder to save the folds (here named, 'EmoFolds')\n","emo_object = MultilabelStratifiedKFold(n_splits=5, random_state=None)\n","fold = 0\n","train_indices_emo = []\n","test_indices_emo =  []\n","for train_index, test_index in emo_object.split(X_emo_bt, y_emo):\n","  fold = fold + 1\n","  print (\"Fold \",fold,\":\")\n","  train_indices_emo.append([train_index])\n","  test_indices_emo.append([test_index])\n","  X_train, y_train  = X_emo_bt[train_index], y_emo[train_index]\n","  np.save('EmoFolds/X_train_emo_'+str(fold)+'.npy',X_train)\n","  np.save('EmoFolds/Y_train_emo_'+str(fold)+'.npy',y_train)\n","  del(X_train)\n","  del(y_train)\n","\n","  X_test, y_test = X_emo_bt[test_index], y_emo[test_index]\n","  np.save('EmoFolds/X_test_emo_'+str(fold)+'.npy',X_test)\n","  np.save('EmoFolds/Y_test_emo_'+str(fold)+'.npy',y_test)\n","  del(X_test)\n","  del(y_test)\n","\n","train_indices_emo = np.asarray(train_indices_emo)\n","np.save('EmoFolds/train_indices_emo.npy',train_indices_emo)\n","\n","test_indices_emo = np.asarray(test_indices_emo)\n","np.save('EmoFolds/test_indices_emo.npy',test_indices_emo)\n","\n","del(X_emo_bt)\n","del(y_emo)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EriBCyrEeIQn"},"source":["*Modeling and Evaluation*"]},{"cell_type":"code","metadata":{"id":"GhgPzlzBYbKv"},"source":["def get_model(task1, task2, n1_classes = 3, lw1=0.8, alpha1 = 0.9):\n","\n","    if n1_classes == 2:\n","      activation1 = 'sigmoid'\n","    else:\n","      activation1 = 'softmax'\n","    \n","    lw1 = lw1           # loss weights\n","    lw2 = 1-lw1\n","    alpha1 = alpha1     # relative weights controlling contribution of task specific and shared encoder to primary task\n","    alpha2 = 1-alpha1\n","\n","    inp1 = Input(shape=(117,768))      # metoo\n","    inp2 = Input(shape = (70,768))      # emo\n","\n","    p1 = Bidirectional(LSTM(units=128, dropout=0.4, recurrent_dropout=0.4, activation='tanh',return_sequences=True))(inp1)    # separate stacked encoder for primary task\n","    p1 = Bidirectional(LSTM(units=128, dropout=0.4, recurrent_dropout=0.4, activation='tanh',return_sequences=True))(p1)\n","\n","    shared_encoder1 = Bidirectional(LSTM(units=128, dropout=0.4, recurrent_dropout=0.4, activation='tanh',return_sequences=True))      # shared stacked encoder for both tasks\n","    shared_encoder2 = Bidirectional(LSTM(units=128, dropout=0.4, recurrent_dropout=0.4, activation='tanh',return_sequences=True))\n","\n","    s1 = shared_encoder1(inp1)        # shared encoder with metoo inputs\n","    s1 = shared_encoder2(s1)\n","\n","    s2 = shared_encoder1(inp2)         # shared encoder with emotion inputs\n","    s2 = shared_encoder2(s2)\n","\n","    p1 = Lambda(lambda x: x * alpha1)(p1)    \n","    s1 = Lambda(lambda x: x * alpha2)(s1)\n","\n","    x1 = keras.layers.add([p1,s1])          # summed up states from primary and shared encoders for primary task\n","    \n","    t1 = Bidirectional(LSTM(units=256, dropout=0.4, recurrent_dropout=0.4, activation='tanh',return_sequences=False))(x1)\n","    t2 = Bidirectional(LSTM(units=256, dropout=0.4, recurrent_dropout=0.4, activation='tanh',return_sequences=False))(s2)\n","    t1 = Dropout(0.3)(t1)\n","    t2 = Dropout(0.3)(t2)\n","\n","    fc1 = Dense(128, activation=\"relu\")(t1)\n","    fc2 = Dense(128, activation=\"relu\")(t2)\n","    final1 = Dense(3, activation=activation1,name = task1)(fc1)\n","    final2 = Dense(11, activation='sigmoid',name = task2)(fc2)\n","\n","    model1 = Model(inputs=inp1,outputs=final1)\n","    model2 = Model(inputs=inp2,outputs=final2)\n","    opt = tf.keras.optimizers.Adam(learning_rate=0.001,beta_1=0.9, beta_2=0.999, amsgrad=False)\n","\n","    if n1_classes==2:\n","     model1.compile(loss = binary_focal_loss(gamma=2.0, alpha=0.25),optimizer=opt, metrics=['acc'],loss_weights = {task1:lw1} )\n","    else:\n","     model1.compile(loss = categorical_focal_loss(gamma=2.0, alpha=0.25),optimizer=opt, metrics=['acc'],loss_weights = {task1:lw1} )\n","    \n","    model2.compile(loss = 'binary_crossentropy',optimizer=opt,metrics=['acc'],loss_weights={task2:lw2})\n","    print(model1.summary())\n","    print(model2.summary())\n","    return model1,model2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-06SGQpPZScI"},"source":["def get_l1_train_test(k=5, task1='stance', task2='emotion', n1_classes=3, lw1 = 0.8):\n","    \n","    F1_macro1 = []\n","    P_macro1= []\n","    R_macro1 = []\n","    F1_micro1= []\n","    P_micro1= []\n","    R_micro1= []\n","    F1_weighted1 = []\n","    P_weighted1= []\n","    R_weighted1= []\n","\n","    F1_macro2 = []\n","    P_macro2= []\n","    R_macro2 = []\n","    F1_micro2 = []\n","    P_micro2 = []\n","    R_micro2 = []\n","    F1_weighted2 = []\n","    P_weighted2= []\n","    R_weighted2= []\n","    fold = 0\n","    for j in range(k):\n","        \n","        fold = fold + 1\n","        epochs = 20\n","        batch_size = 128\n","        print (\"Fold \",fold,\":\")\n","        X_train1, y_train1  =   np.load('MeTooFolds/X_train_metoo_'+str(fold)+'.npy') ,  np.load('MeTooFolds/Y_train_'+str(task1)+'_'+str(fold)+'.npy')\n","        X_test1, y_test1 =   np.load('MeTooFolds/X_test_metoo_'+str(fold)+'.npy') ,  np.load('MeTooFolds/Y_test_'+str(task1)+'_'+str(fold)+'.npy')\n","\n","        X_train2, y_train2   = np.load('EmoFolds/X_train_emo_'+str(fold)+'.npy'),np.load('EmoFolds/Y_train_emo_'+str(fold)+'.npy')\n","        X_test2, y_test2  = np.load('EmoFolds/X_test_emo_'+str(fold)+'.npy'),np.load('EmoFolds/Y_test_emo_'+str(fold)+'.npy')\n","          \n","        model1,model2 = get_model(task1=task1, task2=task2, n1_classes=n1_classes, lw1=lw1, alpha=0.9)\n","\n","        y_train1 = np.eye(n1_classes)[y_train1]\n","\n","        for epoch in range(epochs):\n","          print('epoch:',epoch+1)\n","          history1 = model1.fit(X_train1,y_train1, epochs=1, batch_size = batch_size, verbose=1)\n","          history2 = model2.fit(X_train2,y_train2, epochs=1, batch_size = batch_size, verbose=1) \n","\n","        probs1 = model1.predict(X_test1, batch_size=batch_size, verbose=1)\n","        probs2 = model2.predict(X_test2,batch_size=batch_size,verbose =1)\n","        preds1 = np.argmax(probs1, axis=1)\n","        preds2 = (probs2 >= 0.5).astype(int)       \n","        \n","        print(\"METRICS FOR TASK 1:\" + str(task1))\n","        f1_score_macro1 = metrics.f1_score(y_test1, preds1, average='macro')\n","        p_score_macro1 = metrics.precision_score(y_test1, preds1, average='macro')\n","        r_score_macro1 = metrics.recall_score(y_test1, preds1, average='macro')\n","        f1_score_micro1 = metrics.f1_score(y_test1, preds1, average='micro')\n","        p_score_micro1 = metrics.precision_score(y_test1, preds1, average='micro')\n","        r_score_micro1 = metrics.recall_score(y_test1, preds1, average='micro')\n","        f1_score_weighted1 = metrics.f1_score(y_test1, preds1, average='weighted')\n","        p_score_weighted1 = metrics.precision_score(y_test1, preds1, average='weighted')\n","        r_score_weighted1 = metrics.recall_score(y_test1, preds1, average='weighted')\n","        print (\"F1 Macro1: \",f1_score_macro1, \" P Macro1: \", p_score_macro1, \" R Macro1: \",r_score_macro1)\n","        print (\"F1 Micro1: \",f1_score_micro1, \" P Micro1: \", p_score_micro1, \" R Micro1: \",r_score_micro1)\n","        print (\"F1 Weighted1: \",f1_score_weighted1, \" P Weighted1: \", p_score_weighted1, \" R Weighted1: \",r_score_weighted1)\n","        \n","        print (metrics.confusion_matrix(y_test1, preds1))\n","        print (metrics.classification_report(y_test1, preds1))\n","        F1_macro1.append(f1_score_macro1)\n","        P_macro1.append(p_score_macro1)\n","        R_macro1.append(r_score_macro1)\n","        F1_micro1.append(f1_score_micro1)\n","        P_micro1.append(p_score_micro1)\n","        R_micro1.append(r_score_micro1)\n","        F1_weighted1.append(f1_score_weighted1)\n","        P_weighted1.append(p_score_weighted1)\n","        R_weighted1.append(r_score_weighted1)\n","\n","        print(\"METRICS FOR TASK 2:\" + str(task2))\n","        f1_score_macro2 = metrics.f1_score(y_test2, preds2, average='macro')\n","        p_score_macro2 = metrics.precision_score(y_test2, preds2, average='macro')\n","        r_score_macro2 = metrics.recall_score(y_test2, preds2, average='macro')\n","        f1_score_micro2 = metrics.f1_score(y_test2, preds2, average='micro')\n","        p_score_micro2 = metrics.precision_score(y_test2, preds2, average='micro')\n","        r_score_micro2 = metrics.recall_score(y_test2, preds2, average='micro')\n","        f1_score_weighted2 = metrics.f1_score(y_test2, preds2, average='weighted')\n","        p_score_weighted2 = metrics.precision_score(y_test2, preds2, average='weighted')\n","        r_score_weighted2 = metrics.recall_score(y_test2, preds2, average='weighted')\n","        print (\"F1 Macro2: \",f1_score_macro2, \" P Macro2: \", p_score_macro2, \" R Macro2: \",r_score_macro2)\n","        print (\"F1 Micro2: \",f1_score_micro2, \" P Micro2: \", p_score_micro2, \" R Micro2: \",r_score_micro2)\n","        print (\"F1 Weighted2: \",f1_score_weighted2, \" P Weighted2: \", p_score_weighted2, \" R Weighted2: \",r_score_weighted2)\n","\n","        print(\"Hamming loss = \",hamming_loss(y_test2,preds2))\n","        print (metrics.classification_report(y_test2, preds2))\n","        F1_macro2.append(f1_score_macro2)\n","        P_macro2.append(p_score_macro2)\n","        R_macro2.append(r_score_macro2)\n","        F1_micro2.append(f1_score_micro2)\n","        P_micro2.append(p_score_micro2)\n","        R_micro2.append(r_score_micro2)\n","        F1_weighted2.append(f1_score_weighted2)\n","        P_weighted2.append(p_score_weighted2)\n","        R_weighted2.append(r_score_weighted2)\n","\n","        del(X_train1)\n","        del(X_test1)\n","        del(X_train2)\n","        del(X_test2)\n","        \n","        del(y_train1)\n","        del(y_train2)\n","        del(y_test1)\n","        del(y_test2)\n","\n","    print('FINAL RESULTS FOR TASK 1:'+str(task1))\n","    print (\" Macro - Mean and Dev-  F1: \",np.mean(F1_macro1),\"(\",np.std(F1_macro1),\") P: \",np.mean(P_macro1),\" (\",np.std(P_macro1),\") R: \",np.mean(R_macro1),\" (\",np.std(R_macro1),\")\")\n","    print (\" Micro -  Mean F1 Dev-  F1: \",np.mean(F1_micro1),\"(\",np.std(F1_micro1),\") P: \",np.mean(P_micro1),\" (\",np.std(P_micro1),\") R: \",np.mean(R_micro1),\" (\",np.std(R_micro1),\")\")\n","    print (\" Weighted - Mean and Dev-  F1: \",np.mean(F1_weighted1),\"(\",np.std(F1_weighted1),\") P: \",np.mean(P_weighted1),\" (\",np.std(P_weighted1),\") R: \",np.mean(R_weighted1),\" (\",np.std(R_weighted1),\")\")\n","\n","    print('FINAL RESULTS FOR TASK 2:'+str(task2))\n","    print (\" Macro - Mean and Dev-  F1: \",np.mean(F1_macro2),\"(\",np.std(F1_macro2),\") P: \",np.mean(P_macro2),\" (\",np.std(P_macro2),\") R: \",np.mean(R_macro2),\" (\",np.std(R_macro2),\")\")\n","    print (\" Micro -  Mean F1 Dev-  F1: \",np.mean(F1_micro2),\"(\",np.std(F1_micro2),\") P: \",np.mean(P_micro2),\" (\",np.std(P_micro2),\") R: \",np.mean(R_micro2),\" (\",np.std(R_micro2),\")\")\n","    print (\" Weighted - Mean and Dev-  F1: \",np.mean(F1_weighted2),\"(\",np.std(F1_weighted2),\") P: \",np.mean(P_weighted2),\" (\",np.std(P_weighted2),\") R: \",np.mean(R_weighted2),\" (\",np.std(R_weighted2),\")\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3S6UoR9EcOzG"},"source":["if task1 == 'stance' or task1 == 'hatespeech':\n","  n1_classes = 3\n","elif task1 == 'sarcasm':\n","  n1_classes == 2\n","elif task1 == 'dialogue':\n","  n1_classes = 4\n","\n","# Sample evaluation for 'STANCE' (with loss weight 0.8) and 'EMOTION' (with loss weight 0.2) multi task learning. Change the inputs accordingly for different tasks and loss weights\n","task1 = 'stance'\n","task2 = 'emotion'\n","lw1 = 0.8\n","get_l1_train_test(k=5, task1=task1, task2=task2, n1_classes=n1_classes, lw1=lw1)"],"execution_count":null,"outputs":[]}]}